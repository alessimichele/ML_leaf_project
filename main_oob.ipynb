{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf5eaed3",
   "metadata": {},
   "source": [
    "## Preparing the dataset\n",
    "- load the data\n",
    "- rename the columns\n",
    "- split the dataset into train and test (test will be used only at the very final step)\n",
    "- separate the independent variables from the target variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dde6e234",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# load the data\n",
    "leaf = pd.read_csv(\"leaf.csv\" , header = None)\n",
    "\n",
    "# rename the columns\n",
    "names = np.array([\"Class\", \"Specimen\", \"Eccentricity\", \"Aspect Ratio\", \"Elongation\",\n",
    "                  \"Solidity\", \"Stochastic Convexity\", \"Isoperimetric Factor\",\n",
    "                  \"Maximal Indentation Depth \",\"Lobedness\",\"Average Intensity\",\n",
    "                  \"Average Contrast\",\"Smoothness\",\"Third moment\",\"Uniformity\",\"Entropy\"])\n",
    "leaf.columns = names\n",
    "\n",
    "data = leaf.loc[:, leaf.columns != 'Specimen']\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# split the dataset into train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(data, random_state = 5, test_size = 0.2, stratify = data['Class'])\n",
    "train.index = np.linspace(0, len(train)-1, len(train), dtype = 'int')\n",
    "test.index = np.linspace(0, len(test)-1, len(test), dtype = 'int')\n",
    "\n",
    "\n",
    "# separate the independent variables from the target variables\n",
    "X = train.loc[:, train.columns != 'Class']\n",
    "y = train['Class']\n",
    "# X = X_train\n",
    "# y = y_train\n",
    "\"\"\"\n",
    "X = data.loc[:, data.columns != 'Class']\n",
    "y = data['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5158826",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "plt.figure(figsize = (18,8))\n",
    "y.value_counts().plot(kind = \"bar\")\n",
    "plt.xlabel('Classes')\n",
    "plt.ylabel('Observations per class')\n",
    "\n",
    "plt.savefig('unbalanced_classes.png', dpi=1500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc36c998",
   "metadata": {},
   "source": [
    "## Building some useful tools\n",
    "- declaring two cross-validation loop: \n",
    "    - outer stratified cv-loop\n",
    "    - inner non-stratified cv-loop\n",
    "- set the scoring we'd like to compute (this tuple will be used inside 'do_cross_validation')\n",
    "- definition of do_cross_validation: it evaluate metric(s) by cross-validation and also record fit/score times, then print result(s)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d147e5f",
   "metadata": {},
   "source": [
    "### Decide the metric to use\n",
    "Micro average F1 score performs worse on imbalanced datasets than macro average F1 score. The reason for this is because micro F1 gives equal importance to each observation, whilst macro F1 gives each class equal importance.\n",
    "When micro F1 score gives equal importance to each observation this means that when the classes are imbalanced, those classes with more observations will have a larger impact on the final score. Resulting in a final score which hides the performance of the minority classes and amplifies the majority.\n",
    "On the other hand, macro F1 score gives equal importance to each class. This means that a majority class will contribute equally along with the minority, allowing macro f1 to still return objective results on imbalanced datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c0306c",
   "metadata": {},
   "source": [
    "## Stratified?\n",
    "- Paper: https://arxiv.org/pdf/1811.12808.pdf\n",
    "- Code: https://github.com/rasbt/model-eval-article-supplementary/blob/master/code/nested_cv_code.ipynb\n",
    "\n",
    "Here he uses stratified K Fold both for inner and outer CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e0e6c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, cross_val_score, KFold, LeaveOneOut, cross_validate\n",
    "\n",
    "# declaring two cross-validation loop\n",
    "inner_cv = StratifiedKFold(n_splits = 4) # random_state = 10)\n",
    "outer_cv = StratifiedKFold(n_splits = 5, shuffle = True, random_state=10)\n",
    "\n",
    "# set the scoring\n",
    "scoring = ('f1_macro', 'balanced_accuracy')\n",
    "# definition of do_cross_validation\n",
    "def do_cross_validation(clf,X=X, y=y, print_model=True, scoring = scoring):\n",
    "    cv = cross_validate(clf, X, y, scoring=scoring, cv= outer_cv, return_train_score=False)\n",
    "\n",
    "    for i in range(len(scoring)):\n",
    "        scores = ' + '.join(f'{s:.2f}' for s in cv['test_' + scoring[i]])\n",
    "        mean_ = cv['test_' + scoring[i]].mean()\n",
    "        msg = f'Cross-validated {scoring[i]}: ({scores}) / {outer_cv.n_splits} = {mean_:.2f}'\n",
    "        if print_model:\n",
    "            msg = f'{clf}:\\n\\t{msg}\\n'\n",
    "        print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32580686",
   "metadata": {},
   "source": [
    "## Train Single tree \n",
    "Use 'min_samples_leaf' and impurity measures ('criterion').\n",
    "\n",
    "The paper, \"An empirical study on hyperparameter tuning of decision trees\" states that the ideal min_samples_leaf values tend to be between 1 to 20 (https://arxiv.org/pdf/1812.02207.pdf)\n",
    "\n",
    "\" I think one exception to this is when you have an imbalanced class problem because then the regions in which the minority class will be in majority will be very small so you should go with a lower value.\" (https://towardsdatascience.com/how-to-tune-a-decision-tree-f03721801680#:~:text=further%20in%20min_impurity_decrease.-,The%20paper%2C%20An%20empirical%20study%20on%20hyperparameter%20tuning%20of%20decision,20%20for%20the%20CART%20algorithm.)\n",
    "\n",
    "Our dataset is not so large so I suggest to use not so big values for 'min_samples_leaf'.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/tree.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "687b29be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV(cv=StratifiedKFold(n_splits=4, random_state=None, shuffle=False),\n",
      "             estimator=DecisionTreeClassifier(random_state=0), n_jobs=4,\n",
      "             param_grid={'criterion': ['gini', 'entropy'],\n",
      "                         'min_samples_leaf': [1, 2, 5, 10, 15]},\n",
      "             scoring='balanced_accuracy'):\n",
      "\tCross-validated f1_macro: (0.70 + 0.61 + 0.47 + 0.60 + 0.71) / 5 = 0.62\n",
      "\n",
      "GridSearchCV(cv=StratifiedKFold(n_splits=4, random_state=None, shuffle=False),\n",
      "             estimator=DecisionTreeClassifier(random_state=0), n_jobs=4,\n",
      "             param_grid={'criterion': ['gini', 'entropy'],\n",
      "                         'min_samples_leaf': [1, 2, 5, 10, 15]},\n",
      "             scoring='balanced_accuracy'):\n",
      "\tCross-validated balanced_accuracy: (0.73 + 0.63 + 0.53 + 0.62 + 0.73) / 5 = 0.65\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# tree inner loop\n",
    "param_grid={'min_samples_leaf': [1, 2, 5, 10, 15] , 'criterion': [\"gini\", \"entropy\"]}\n",
    "clf_grid = GridSearchCV(DecisionTreeClassifier(random_state = 0), scoring = 'balanced_accuracy', param_grid=param_grid, cv = inner_cv, n_jobs=4)\n",
    "# tree outer loop\n",
    "do_cross_validation(clf_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "78478151",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07547184905645282"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std([0.73, 0.63, 0.53, 0.62, 0.73])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4001a0b7",
   "metadata": {},
   "source": [
    "## Train RandomForest\n",
    "- train a RandomForest by a two nested CV loop. \n",
    "    - in the outer loop we estimate the score indexes using 'do_cross_validation'\n",
    "    - in the inner loop we to GridSearch\n",
    "\n",
    "The main parameters to adjust when using Random Forest method is n_estimators and max_features.\n",
    "\n",
    "Sklearn documentation (https://scikit-learn.org/stable/modules/ensemble.html#parameters) suggests that the option 'sqrt' (this option will take square root of the total number of features in individual each split) for 'max_features' is generally good for classification problems ('sqrt' should be the default value).\n",
    "For 'n_estimators' the larger the better, but also the longer it will take to compute so we can choose some random  high values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "673d580c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV(cv=StratifiedKFold(n_splits=4, random_state=None, shuffle=False),\n",
      "             estimator=RandomForestClassifier(random_state=0), n_jobs=4,\n",
      "             param_grid={'max_features': ['sqrt'],\n",
      "                         'n_estimators': [100, 200, 500, 750, 1000]},\n",
      "             scoring='balanced_accuracy'):\n",
      "\tCross-validated f1_macro: (0.81 + 0.71 + 0.75 + 0.85 + 0.77) / 5 = 0.78\n",
      "\n",
      "GridSearchCV(cv=StratifiedKFold(n_splits=4, random_state=None, shuffle=False),\n",
      "             estimator=RandomForestClassifier(random_state=0), n_jobs=4,\n",
      "             param_grid={'max_features': ['sqrt'],\n",
      "                         'n_estimators': [100, 200, 500, 750, 1000]},\n",
      "             scoring='balanced_accuracy'):\n",
      "\tCross-validated balanced_accuracy: (0.82 + 0.74 + 0.77 + 0.85 + 0.81) / 5 = 0.80\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# random forest inner loop\n",
    "param_grid={'max_features': ['sqrt'], 'n_estimators':[ 100, 200, 500, 750 ,1000]}\n",
    "clf_grid = GridSearchCV(RandomForestClassifier(random_state=0), scoring = 'balanced_accuracy', param_grid=param_grid, cv = inner_cv, n_jobs=4)\n",
    "# random forest outer loop\n",
    "do_cross_validation(clf_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa384a6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.026532998322843223"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std([0.78,0.73,0.70,0.74,0.72])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d113ba18",
   "metadata": {},
   "source": [
    "## Standardization\n",
    "\n",
    "before moving to SVM and kNN, we need to standardize our independent variable. Since we are using cross validation also to assess our final performance metric we need to ensure that standardization is performed in each fold and not on the whole dataset.\\\n",
    "In order to do this we need to use the sklearn `pipeline` in conjunction with nested cross validation as before.\\\n",
    "**Performing standardization on the whole set would be wrong as we'd have information leakage from the data we use to perform our cross validation**\\\n",
    "\n",
    "When you use parameter_grid remember that the parameters must have the name of the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72c9773b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b226f4",
   "metadata": {},
   "source": [
    "## Train SVM with Gaussian Kernel \n",
    "- train a SVM by a two nested CV loop. \n",
    "    - in the outer loop we estimate the score indexes using 'do_cross_validation'\n",
    "    - in the inner loop we to GridSearch\n",
    "\n",
    "Hyper-paramters to consider in the grid search:\n",
    " - `gamma`: use a set of float values, sklearn documentation suggests using exponentially spaced ones (and so do other tutorials).\n",
    " - `C`: use a set of float values, sklearn documentation suggests using exponentially spaced ones (and so do other tutorials)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7cf38c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "pipeline_SVC = Pipeline([('scaler', StandardScaler()),\n",
    "                        ('classifier', SVC(kernel='rbf', probability = True))]) #If we choose some scoring without probability set probability to FALSE\n",
    "param_grid = {'classifier__C': np.logspace(-2, 5, 8), 'classifier__gamma' : np.logspace(-4, 3, 8)}\n",
    "grid_search = GridSearchCV(pipeline_SVC, param_grid=param_grid, scoring = 'balanced_accuracy', cv = inner_cv, n_jobs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "454ff202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV(cv=StratifiedKFold(n_splits=4, random_state=None, shuffle=False),\n",
      "             estimator=Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                                       ('classifier', SVC(probability=True))]),\n",
      "             n_jobs=4,\n",
      "             param_grid={'classifier__C': array([1.e-02, 1.e-01, 1.e+00, 1.e+01, 1.e+02, 1.e+03, 1.e+04, 1.e+05]),\n",
      "                         'classifier__gamma': array([1.e-04, 1.e-03, 1.e-02, 1.e-01, 1.e+00, 1.e+01, 1.e+02, 1.e+03])},\n",
      "             scoring='balanced_accuracy'):\n",
      "\tCross-validated f1_macro: (0.86 + 0.72 + 0.77 + 0.69 + 0.71) / 5 = 0.75\n",
      "\n",
      "GridSearchCV(cv=StratifiedKFold(n_splits=4, random_state=None, shuffle=False),\n",
      "             estimator=Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                                       ('classifier', SVC(probability=True))]),\n",
      "             n_jobs=4,\n",
      "             param_grid={'classifier__C': array([1.e-02, 1.e-01, 1.e+00, 1.e+01, 1.e+02, 1.e+03, 1.e+04, 1.e+05]),\n",
      "                         'classifier__gamma': array([1.e-04, 1.e-03, 1.e-02, 1.e-01, 1.e+00, 1.e+01, 1.e+02, 1.e+03])},\n",
      "             scoring='balanced_accuracy'):\n",
      "\tCross-validated balanced_accuracy: (0.88 + 0.73 + 0.78 + 0.73 + 0.73) / 5 = 0.77\n",
      "\n"
     ]
    }
   ],
   "source": [
    "do_cross_validation(grid_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e8160115",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.058309518948453015"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std([0.88, 0.73, 0.78, 0.73, 0.73])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aca1ba3",
   "metadata": {},
   "source": [
    "## Train SVM with Linear Kernel \n",
    "- train a SVM by a two nested CV loop. \n",
    "    - in the outer loop we estimate the score indexes using 'do_cross_validation'\n",
    "    - in the inner loop we to GridSearch\n",
    "\n",
    "Hyper-paramters to consider in the grid search:\n",
    " - `gamma`: use a set of float values, sklearn documentation suggests using exponentially spaced ones (and so do other tutorials).\n",
    " - `C`: use a set of float values, sklearn documentation suggests using exponentially spaced ones (and so do other tutorials)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987d8379",
   "metadata": {},
   "source": [
    "### LinearSVC: possible alternative, but discard since ther's no built in probability parameter\n",
    "Linear Support Vector Classification.\n",
    "\n",
    "Similar to SVC with parameter kernel=’linear’, but implemented in terms of liblinear rather than libsvm, so it has more flexibility in the choice of penalties and loss functions and should scale better to large numbers of samples.\n",
    "\n",
    "This class supports both dense and sparse input and the multiclass support is handled according to a one-vs-the-rest scheme.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443f72a2",
   "metadata": {},
   "source": [
    "https://medium.com/@myselfaman12345/c-and-gamma-in-svm-e6cee48626be\n",
    "    \n",
    "After multiple trials, we get the higher score with that value of C parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3911a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "pipeline_SVC = Pipeline([('scaler', StandardScaler()),\n",
    "                        ('classifier', SVC(kernel='linear', probability=True))]) #If we choose some scoring without probability set probability to FALSE\n",
    "param_grid = {'classifier__C': np.logspace(-3, 10, 10), 'classifier__decision_function_shape': ['ovo','ovr']}\n",
    "grid_search = GridSearchCV(pipeline_SVC, param_grid=param_grid, scoring = 'balanced_accuracy', cv = inner_cv, n_jobs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3d8950f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV(cv=StratifiedKFold(n_splits=4, random_state=None, shuffle=False),\n",
      "             estimator=Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                                       ('classifier',\n",
      "                                        SVC(kernel='linear',\n",
      "                                            probability=True))]),\n",
      "             n_jobs=4,\n",
      "             param_grid={'classifier__C': array([1.00000000e-03, 2.78255940e-02, 7.74263683e-01, 2.15443469e+01,\n",
      "       5.99484250e+02, 1.66810054e+04, 4.64158883e+05, 1.29154967e+07,\n",
      "       3.59381366e+08, 1.00000000e+10]),\n",
      "                         'classifier__decision_function_shape': ['ovo', 'ovr']},\n",
      "             scoring='balanced_accuracy'):\n",
      "\tCross-validated f1_macro: (0.88 + 0.70 + 0.81 + 0.76 + 0.73) / 5 = 0.78\n",
      "\n",
      "GridSearchCV(cv=StratifiedKFold(n_splits=4, random_state=None, shuffle=False),\n",
      "             estimator=Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                                       ('classifier',\n",
      "                                        SVC(kernel='linear',\n",
      "                                            probability=True))]),\n",
      "             n_jobs=4,\n",
      "             param_grid={'classifier__C': array([1.00000000e-03, 2.78255940e-02, 7.74263683e-01, 2.15443469e+01,\n",
      "       5.99484250e+02, 1.66810054e+04, 4.64158883e+05, 1.29154967e+07,\n",
      "       3.59381366e+08, 1.00000000e+10]),\n",
      "                         'classifier__decision_function_shape': ['ovo', 'ovr']},\n",
      "             scoring='balanced_accuracy'):\n",
      "\tCross-validated balanced_accuracy: (0.88 + 0.73 + 0.82 + 0.76 + 0.75) / 5 = 0.79\n",
      "\n"
     ]
    }
   ],
   "source": [
    "do_cross_validation(grid_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "78cbb0a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.054918120870983925"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std([0.88 , 0.73 , 0.82, 0.76, 0.75])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0720ccb",
   "metadata": {},
   "source": [
    "## Train kNN\n",
    "- train a kNN by a two nested CV loop. \n",
    "    - in the outer loop we estimate the score indexes using 'do_cross_validation'\n",
    "    - in the inner loop we to GridSearch\n",
    "\n",
    "The main hyper-parameters to consider when using kNN method is the distance ('classifier__p') and the number of neighbors ('classifier__n_neighbors')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "04b375b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "pipeline_KNN = Pipeline([('scaler', StandardScaler()),\n",
    "                        ('classifier', KNeighborsClassifier())])\n",
    "\n",
    "# classifier_p = 1 means manhattan_distance (l1), classifier_p = 2 means euclidean_distance (l2)\n",
    "param_grid = {'classifier__n_neighbors': list(range(1,15)), 'classifier__p':[1, 2]}\n",
    "grid_search = GridSearchCV(pipeline_KNN, param_grid=param_grid, scoring = 'balanced_accuracy', cv = inner_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cc3e1ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV(cv=StratifiedKFold(n_splits=4, random_state=None, shuffle=False),\n",
      "             estimator=Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                                       ('classifier', KNeighborsClassifier())]),\n",
      "             param_grid={'classifier__n_neighbors': [1, 2, 3, 4, 5, 6, 7, 8, 9,\n",
      "                                                     10, 11, 12, 13, 14],\n",
      "                         'classifier__p': [1, 2]},\n",
      "             scoring='balanced_accuracy'):\n",
      "\tCross-validated f1_macro: (0.66 + 0.69 + 0.66 + 0.65 + 0.66) / 5 = 0.66\n",
      "\n",
      "GridSearchCV(cv=StratifiedKFold(n_splits=4, random_state=None, shuffle=False),\n",
      "             estimator=Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                                       ('classifier', KNeighborsClassifier())]),\n",
      "             param_grid={'classifier__n_neighbors': [1, 2, 3, 4, 5, 6, 7, 8, 9,\n",
      "                                                     10, 11, 12, 13, 14],\n",
      "                         'classifier__p': [1, 2]},\n",
      "             scoring='balanced_accuracy'):\n",
      "\tCross-validated balanced_accuracy: (0.69 + 0.71 + 0.69 + 0.65 + 0.69) / 5 = 0.69\n",
      "\n"
     ]
    }
   ],
   "source": [
    "do_cross_validation(grid_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0b7c4698",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0195959179422654"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std([0.69, 0.71 , 0.69 , 0.65 , 0.69])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f86331b",
   "metadata": {},
   "source": [
    "## Plot a confusion matrix for the choosen classifier (RF, which has greater balanced accuracy and smaller std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ce0d644-2de1-4600-b55c-143e0ea94abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a77c618f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "param_grid={'max_features': ['sqrt'], 'n_estimators':[ 100, 200, 500, 750 ,1000]}\n",
    "RF = GridSearchCV(RandomForestClassifier(random_state=0), scoring = 'balanced_accuracy', param_grid=param_grid, cv = inner_cv, n_jobs=4)\n",
    "\n",
    "RF = RF.fit(X,y)\n",
    "\"\"\"\n",
    "y_pred = RF.predict(X_test)\n",
    "\n",
    "\n",
    "plt.figure(figsize = (18,8))\n",
    "sns.heatmap(metrics.confusion_matrix(y_test, y_pred), annot = True, xticklabels = y_test.unique(), yticklabels = y_test.unique(), cmap = 'summer')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "\n",
    "plt.savefig('confmat.png', dpi=1500)\n",
    "plt.show()\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "566d1969-ad50-4ce0-8f03-e442709ccbd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(n_estimators=500, random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(n_estimators=500, random_state=0)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(n_estimators=500, random_state=0)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RF.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7ccec170-80ef-4e92-afc0-0d76907ea8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_final = RandomForestClassifier(random_state = 0, oob_score = True, n_estimators = 500) #selected as the best one from above\n",
    "RF_final = RF_final.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2fea07d9-d4a5-498f-aa29-0abce97fe900",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = RF_final.oob_decision_function_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f0281a36-d9d9-4889-a9b3-a663fecbcef0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 22, 23,\n",
       "       24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f0eaf331-86ea-4d56-841e-99e04d62386a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010638</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010638</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005319</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.069149</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005319</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.418079</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.169492</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005650</td>\n",
       "      <td>0.056497</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.853933</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005618</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028090</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.044944</td>\n",
       "      <td>0.039326</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.047059</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.094118</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005882</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029412</td>\n",
       "      <td>0.035294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.704918</td>\n",
       "      <td>0.005464</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005464</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016393</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.163934</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.032787</td>\n",
       "      <td>0.010929</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015789</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010526</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010526</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.915789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.070652</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005435</td>\n",
       "      <td>0.826087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016393</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.956284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016575</td>\n",
       "      <td>0.149171</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005525</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.430939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005747</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028736</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.034483</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005747</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005747</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.310345</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>340 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6   \\\n",
       "0    0.000000  0.010638  0.000000  0.010638  0.000000  0.000000  0.000000   \n",
       "1    0.418079  0.000000  0.000000  0.016949  0.000000  0.000000  0.000000   \n",
       "2    0.853933  0.000000  0.000000  0.005618  0.000000  0.000000  0.000000   \n",
       "3    0.588235  0.000000  0.000000  0.047059  0.000000  0.000000  0.000000   \n",
       "4    0.704918  0.005464  0.000000  0.000000  0.000000  0.000000  0.005464   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "335  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "336  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "337  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "338  0.000000  0.000000  0.000000  0.000000  0.016575  0.149171  0.000000   \n",
       "339  0.000000  0.000000  0.005747  0.000000  0.000000  0.028736  0.000000   \n",
       "\n",
       "           7         8         9   ...        20        21        22  \\\n",
       "0    0.000000  0.005319  0.000000  ...  0.069149  0.000000  0.000000   \n",
       "1    0.000000  0.000000  0.000000  ...  0.169492  0.000000  0.000000   \n",
       "2    0.000000  0.000000  0.000000  ...  0.028090  0.000000  0.000000   \n",
       "3    0.000000  0.000000  0.000000  ...  0.094118  0.000000  0.005882   \n",
       "4    0.000000  0.000000  0.000000  ...  0.016393  0.000000  0.163934   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "335  0.015789  0.000000  0.010526  ...  0.000000  0.010526  0.000000   \n",
       "336  0.000000  0.000000  0.070652  ...  0.000000  0.010870  0.000000   \n",
       "337  0.000000  0.000000  0.016393  ...  0.000000  0.000000  0.000000   \n",
       "338  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "339  0.034483  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "\n",
       "           23        24        25        26        27        28        29  \n",
       "0    0.000000  0.000000  0.000000  0.005319  0.000000  0.000000  0.000000  \n",
       "1    0.000000  0.000000  0.005650  0.056497  0.000000  0.000000  0.000000  \n",
       "2    0.000000  0.000000  0.044944  0.039326  0.000000  0.000000  0.000000  \n",
       "3    0.000000  0.000000  0.029412  0.035294  0.000000  0.000000  0.000000  \n",
       "4    0.000000  0.000000  0.032787  0.010929  0.000000  0.000000  0.000000  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "335  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.915789  \n",
       "336  0.000000  0.000000  0.000000  0.000000  0.000000  0.005435  0.826087  \n",
       "337  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.956284  \n",
       "338  0.005525  0.000000  0.000000  0.000000  0.000000  0.000000  0.430939  \n",
       "339  0.000000  0.005747  0.000000  0.000000  0.005747  0.000000  0.310345  \n",
       "\n",
       "[340 rows x 30 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs_df = pd.DataFrame(probs)\n",
    "probs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b4a6a8a9-e1df-4310-9f82-08c23d56da4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010638</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010638</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005319</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.069149</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005319</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.418079</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.169492</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005650</td>\n",
       "      <td>0.056497</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.853933</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005618</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028090</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.044944</td>\n",
       "      <td>0.039326</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.047059</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.094118</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005882</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029412</td>\n",
       "      <td>0.035294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.704918</td>\n",
       "      <td>0.005464</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005464</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016393</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.163934</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.032787</td>\n",
       "      <td>0.010929</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015789</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010526</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010526</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.915789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.070652</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005435</td>\n",
       "      <td>0.826087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016393</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.956284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016575</td>\n",
       "      <td>0.149171</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005525</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.430939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005747</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028736</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.034483</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005747</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005747</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.310345</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>340 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           1         2         3         4         5         6         7   \\\n",
       "0    0.000000  0.010638  0.000000  0.010638  0.000000  0.000000  0.000000   \n",
       "1    0.418079  0.000000  0.000000  0.016949  0.000000  0.000000  0.000000   \n",
       "2    0.853933  0.000000  0.000000  0.005618  0.000000  0.000000  0.000000   \n",
       "3    0.588235  0.000000  0.000000  0.047059  0.000000  0.000000  0.000000   \n",
       "4    0.704918  0.005464  0.000000  0.000000  0.000000  0.000000  0.005464   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "335  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "336  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "337  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "338  0.000000  0.000000  0.000000  0.000000  0.016575  0.149171  0.000000   \n",
       "339  0.000000  0.000000  0.005747  0.000000  0.000000  0.028736  0.000000   \n",
       "\n",
       "           8         9         10  ...        27        28        29  \\\n",
       "0    0.000000  0.005319  0.000000  ...  0.069149  0.000000  0.000000   \n",
       "1    0.000000  0.000000  0.000000  ...  0.169492  0.000000  0.000000   \n",
       "2    0.000000  0.000000  0.000000  ...  0.028090  0.000000  0.000000   \n",
       "3    0.000000  0.000000  0.000000  ...  0.094118  0.000000  0.005882   \n",
       "4    0.000000  0.000000  0.000000  ...  0.016393  0.000000  0.163934   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "335  0.015789  0.000000  0.010526  ...  0.000000  0.010526  0.000000   \n",
       "336  0.000000  0.000000  0.070652  ...  0.000000  0.010870  0.000000   \n",
       "337  0.000000  0.000000  0.016393  ...  0.000000  0.000000  0.000000   \n",
       "338  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "339  0.034483  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "\n",
       "           30        31        32        33        34        35        36  \n",
       "0    0.000000  0.000000  0.000000  0.005319  0.000000  0.000000  0.000000  \n",
       "1    0.000000  0.000000  0.005650  0.056497  0.000000  0.000000  0.000000  \n",
       "2    0.000000  0.000000  0.044944  0.039326  0.000000  0.000000  0.000000  \n",
       "3    0.000000  0.000000  0.029412  0.035294  0.000000  0.000000  0.000000  \n",
       "4    0.000000  0.000000  0.032787  0.010929  0.000000  0.000000  0.000000  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "335  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.915789  \n",
       "336  0.000000  0.000000  0.000000  0.000000  0.000000  0.005435  0.826087  \n",
       "337  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.956284  \n",
       "338  0.005525  0.000000  0.000000  0.000000  0.000000  0.000000  0.430939  \n",
       "339  0.000000  0.005747  0.000000  0.000000  0.005747  0.000000  0.310345  \n",
       "\n",
       "[340 rows x 30 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs_df.columns = y.unique()\n",
    "probs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "90f2231b-55a9-47ab-adf6-ceb617a6d9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_oob_pred = probs_df.columns[probs_df.apply(np.argmax, axis = 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7189796d-64e1-4474-b698-08bd5b73c3c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Int64Index([13,  1,  1,  1,  1,  1,  1,  1,  1, 29,\n",
      "            ...\n",
      "            36, 36, 36, 36, 36, 36, 36, 36, 36, 11],\n",
      "           dtype='int64', length=340)\n"
     ]
    }
   ],
   "source": [
    "print(y_oob_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1b6151a7-2d2c-4534-b377-5ae3c6601cc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oob_confusion = np.zeros((30, 3))\n",
    "oob_confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6e2b52ca-d37f-46d3-9419-af97d0045dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(30):\n",
    "    oob_confusion[i][0] = y.unique()[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ffc1f2ee-cba8-4243-8aea-1e510e686c48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  0.],\n",
       "       [ 2.,  0.,  0.],\n",
       "       [ 3.,  0.,  0.],\n",
       "       [ 4.,  0.,  0.],\n",
       "       [ 5.,  0.,  0.],\n",
       "       [ 6.,  0.,  0.],\n",
       "       [ 7.,  0.,  0.],\n",
       "       [ 8.,  0.,  0.],\n",
       "       [ 9.,  0.,  0.],\n",
       "       [10.,  0.,  0.],\n",
       "       [11.,  0.,  0.],\n",
       "       [12.,  0.,  0.],\n",
       "       [13.,  0.,  0.],\n",
       "       [14.,  0.,  0.],\n",
       "       [15.,  0.,  0.],\n",
       "       [22.,  0.,  0.],\n",
       "       [23.,  0.,  0.],\n",
       "       [24.,  0.,  0.],\n",
       "       [25.,  0.,  0.],\n",
       "       [26.,  0.,  0.],\n",
       "       [27.,  0.,  0.],\n",
       "       [28.,  0.,  0.],\n",
       "       [29.,  0.,  0.],\n",
       "       [30.,  0.,  0.],\n",
       "       [31.,  0.,  0.],\n",
       "       [32.,  0.,  0.],\n",
       "       [33.,  0.,  0.],\n",
       "       [34.,  0.,  0.],\n",
       "       [35.,  0.,  0.],\n",
       "       [36.,  0.,  0.]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oob_confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "14ed33b0-82ea-4637-b401-3513f955f01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(340):\n",
    "    if y_oob_pred[i] == y[i]:\n",
    "        for j in range(30):\n",
    "            if y[i] ==  oob_confusion[j][0]:\n",
    "                oob_confusion[j][1] += 1\n",
    "    else:\n",
    "        for k in range(30):\n",
    "            if y[i] ==  oob_confusion[k][0]:\n",
    "                oob_confusion[k][2] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "90ac1bd0-cc02-4db6-b793-67004a9104b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1., 10.,  2.],\n",
       "       [ 2.,  4.,  6.],\n",
       "       [ 3.,  6.,  4.],\n",
       "       [ 4.,  1.,  7.],\n",
       "       [ 5., 12.,  0.],\n",
       "       [ 6.,  8.,  0.],\n",
       "       [ 7.,  8.,  2.],\n",
       "       [ 8., 11.,  0.],\n",
       "       [ 9., 10.,  4.],\n",
       "       [10., 11.,  2.],\n",
       "       [11., 16.,  0.],\n",
       "       [12.,  9.,  3.],\n",
       "       [13.,  8.,  5.],\n",
       "       [14.,  9.,  3.],\n",
       "       [15., 10.,  0.],\n",
       "       [22.,  8.,  4.],\n",
       "       [23., 11.,  0.],\n",
       "       [24., 10.,  3.],\n",
       "       [25.,  8.,  1.],\n",
       "       [26.,  8.,  4.],\n",
       "       [27.,  7.,  4.],\n",
       "       [28.,  8.,  4.],\n",
       "       [29., 12.,  0.],\n",
       "       [30., 11.,  1.],\n",
       "       [31.,  9.,  2.],\n",
       "       [32.,  5.,  6.],\n",
       "       [33.,  7.,  4.],\n",
       "       [34.,  9.,  2.],\n",
       "       [35.,  8.,  3.],\n",
       "       [36.,  9.,  1.]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oob_confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "b6de8daa-ce1a-46ac-89b2-f82465db1d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "oob_confusion2 = np.zeros((30, 30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "314bbd30-d29f-493a-98b1-a02e42a5680b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(y)):\n",
    "    if y_oob_pred[i] == y[i]:\n",
    "        if y[i] <= 14:\n",
    "            oob_confusion2[y[i]-1][y[i]-1] += 1\n",
    "        else:\n",
    "            oob_confusion2[y[i]-7][y[i]-7] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "993d2722-9046-4909-88d7-f9e80bc1ebe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "counted_y = y.value_counts()\n",
    "dic_counts = dict(counted_y)\n",
    "dic_counts = dict(sorted(dic_counts.items(), key=lambda item: item[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "3c2e8b88-d3fa-4b29-b1c4-d10d830dabbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12,\n",
       " 10,\n",
       " 10,\n",
       " 8,\n",
       " 12,\n",
       " 8,\n",
       " 10,\n",
       " 11,\n",
       " 14,\n",
       " 13,\n",
       " 16,\n",
       " 12,\n",
       " 13,\n",
       " 12,\n",
       " 10,\n",
       " 12,\n",
       " 11,\n",
       " 13,\n",
       " 9,\n",
       " 12,\n",
       " 11,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 10]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(dic_counts.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "5330d957-a428-4f71-8269-01f317dd25dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.],\n",
       "       [ 0.,  4.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  6.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0., 12.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  8.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  8.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0., 11.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 20.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 11.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 16.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  9.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  8.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         9.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  8.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0., 11.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0., 10.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  8.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  8.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  7.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  8.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 12.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 11.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  9.,  0.,\n",
       "         0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  5.,\n",
       "         0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         7.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  9.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  8.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  9.]])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oob_confusion2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "5d98a2ac-fd42-4c53-9f10-b1488141d835",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(30):\n",
    "    oob_confusion2[i][i] = oob_confusion2[i][i]/list(dic_counts.values())[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "a6c0b5f9-1ebd-494a-b7cf-18789dc217c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7571379546379544"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(oob_confusion2.diagonal())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e98968ea-18b4-4bbb-9737-25aab15e643a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7735294117647059"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(y_oob_pred == y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6f15b3d0-0312-425e-ab7a-5b43040092c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7735294117647059"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RF_final.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "85cdd6ea-4f96-43a2-b6c3-5d0c80f00afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_oob = probs_df.apply(np.argmax, axis = 1) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "01cae960-9ee1-4695-9076-256c36efb2fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      13\n",
       "1       1\n",
       "2       1\n",
       "3       1\n",
       "4       1\n",
       "       ..\n",
       "335    30\n",
       "336    30\n",
       "337    30\n",
       "338    30\n",
       "339    11\n",
       "Length: 340, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_oob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1f300797-98c9-44ea-80c8-17abaffc7412",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(y_oob == y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "058d9edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "target=[\"14\",\"27\",\"29\",\"26\",\"31\", \"25\",\"12\",\"1\",\"32\",\"24\",\"5\",\"11\",\"33\",\"9\",\"28\",\"35\",\"6\",\"15\",\"36\",\n",
    "       \"4\",\"2\", \"22\", \"13\",\"10\",\"23\",\"8\",\"7\",\"30\",\"3\",\"34\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7241c32a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          14       1.00      1.00      1.00         3\n",
      "          27       0.50      1.00      0.67         2\n",
      "          29       1.00      0.50      0.67         2\n",
      "          26       1.00      0.50      0.67         2\n",
      "          31       1.00      1.00      1.00         2\n",
      "          25       1.00      1.00      1.00         2\n",
      "          12       1.00      1.00      1.00         2\n",
      "           1       1.00      1.00      1.00         2\n",
      "          32       0.50      1.00      0.67         3\n",
      "          24       1.00      0.67      0.80         3\n",
      "           5       0.75      1.00      0.86         3\n",
      "          11       1.00      1.00      1.00         2\n",
      "          33       0.60      1.00      0.75         3\n",
      "           9       0.50      0.50      0.50         2\n",
      "          28       1.00      1.00      1.00         2\n",
      "          35       0.67      1.00      0.80         2\n",
      "           6       1.00      1.00      1.00         2\n",
      "          15       0.50      0.67      0.57         3\n",
      "          36       1.00      1.00      1.00         2\n",
      "           4       0.00      0.00      0.00         2\n",
      "           2       1.00      1.00      1.00         2\n",
      "          22       0.50      0.33      0.40         3\n",
      "          13       1.00      1.00      1.00         2\n",
      "          10       1.00      1.00      1.00         3\n",
      "          23       1.00      1.00      1.00         2\n",
      "           8       0.00      0.00      0.00         2\n",
      "           7       1.00      0.50      0.67         2\n",
      "          30       1.00      1.00      1.00         2\n",
      "           3       0.50      0.50      0.50         2\n",
      "          34       1.00      0.50      0.67         2\n",
      "\n",
      "    accuracy                           0.79        68\n",
      "   macro avg       0.80      0.79      0.77        68\n",
      "weighted avg       0.79      0.79      0.77        68\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred, zero_division=0, target_names = target ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
